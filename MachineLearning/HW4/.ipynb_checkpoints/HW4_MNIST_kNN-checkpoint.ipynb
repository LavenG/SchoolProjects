{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI-UA 0473 - Introduction to Machine Learning\n",
    "## Wednesday, March 22, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbour - The Lazy Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task for this homework is again that of handwritten digit recognition on the MNIST dataset. From the huge corpus of 70k images, I have sampled 3k images uniformly i.e. there are 300 images in each class.\n",
    "\n",
    "I have split the 3k samples into training set (2500 points) and test set (500 points) randomly. This experimental dataset is the same for all and I have provided the pickle files with the corresponding data.\n",
    "\n",
    "In this homework, you will be experimenting with different distance functions and $k$. No complicated (or simple, for some) coding involved. Only play with the distance function and $k$.\n",
    "\n",
    "Conduct your experiments in a principled way to decide the best distance function according to you and the optimal value of $k$ for the best distance function. Use the accuracy score as the evaluation metric.\n",
    "\n",
    "Explain your approach clearly in the write-up.\n",
    "\n",
    "Report your experiment results using plots (preferably) or tables with the accuracies in the write-up.\n",
    "\n",
    "(EXTRA CREDITS) Come up with your own distance function, explain it clearly and provide the results from your distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.spatial.distance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training Data: ', (2500, 784))\n",
      "('Test Data: ', (500, 784))\n",
      "('Training Label Distribution: ', array([249, 253, 253, 246, 249, 248, 245, 244, 261, 252]))\n",
      "('Test Label Distribution: ', array([51, 47, 47, 54, 51, 52, 55, 56, 39, 48]))\n"
     ]
    }
   ],
   "source": [
    "training_data = pickle.load(open('training_data_hw4.p', 'rb'))\n",
    "training_labels = pickle.load(open('training_labels_hw4.p', 'rb'))\n",
    "\n",
    "test_data = pickle.load(open('test_data_hw4.p', 'rb'))\n",
    "test_labels = pickle.load(open('test_labels_hw4.p', 'rb'))\n",
    "\n",
    "\n",
    "print ('Training Data: ', training_data.shape)\n",
    "print ('Test Data: ', test_data.shape)\n",
    "\n",
    "print ('Training Label Distribution: ', np.bincount(training_labels))\n",
    "print ('Test Label Distribution: ', np.bincount(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1066df5d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nivADQRF+ICjCDwT1B0C5OMm7sqL8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1066b4b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_data[100].reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_classify(x_, x, y, metric='euclidean', k=1):\n",
    "    if len(x_.shape) < 2:\n",
    "        x_ = x_.reshape([1,-1])\n",
    "        \n",
    "    dists = scipy.spatial.distance.cdist(x_, x, metric)\n",
    "    \n",
    "    sidx = np.argpartition(dists, k, axis=1)[:,:k]\n",
    "    \n",
    "    y_ = np.zeros(len(x_))\n",
    "    for ii, xx_ in enumerate(x_):\n",
    "        yy_, yc_ = np.unique(y[sidx[ii,:]], return_counts=True)\n",
    "        y_[ii] = yy_[np.argmax(yc_)]\n",
    "        \n",
    "    return y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling - Play Area - Experiment with different distance functions and k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.816, Test accuracy 0.792\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "metric = 'euclidean'\n",
    "\n",
    "tra_acc = np.sum(knn_classify(training_data, training_data, training_labels, metric, k) == training_labels) / np.float(len(training_labels))\n",
    "tes_acc = np.sum(knn_classify(test_data, training_data, training_labels, metric, k) == test_labels) / np.float(len(test_labels))\n",
    "\n",
    "print 'Training accuracy {}, Test accuracy {}'.format(tra_acc, tes_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for distance\n",
      "Cosine: Training accuracy 0.856, Test accuracy 0.848\n",
      "Correlation: Training accuracy 0.854, Test accuracy 0.85\n",
      "Testing for k\n",
      "K set to square root: Training accuracy 0.9516, Test accuracy 0.946\n"
     ]
    }
   ],
   "source": [
    "# REPORT THE TEST ACCURACY USING YOUR EXPERIMENTAL SETUP\n",
    "\n",
    "#We first need to pick the best distance function while keeping k constant to compare the results.\n",
    "#Eucledian distance can be bad in high dimensional space because it can't recognize the difference\n",
    "#between the importance of certain points.\n",
    "\n",
    "#In this case cosine distance works better because it measures the similiarity\n",
    "#of vectors with respect to origin\n",
    "print 'Testing for distance'\n",
    "k = 100\n",
    "metric = 'cosine'\n",
    "\n",
    "tra_acc = np.sum(knn_classify(training_data, training_data, training_labels, metric, k) == training_labels) / np.float(len(training_labels))\n",
    "tes_acc = np.sum(knn_classify(test_data, training_data, training_labels, metric, k) == test_labels) / np.float(len(test_labels))\n",
    "\n",
    "print 'Cosine: Training accuracy {}, Test accuracy {}'.format(tra_acc, tes_acc)\n",
    "\n",
    "#We can see that cosine and correlation yield similar results from the test below and so do other\n",
    "#metrics such as sokalsneath \n",
    "\n",
    "k = 100\n",
    "metric = 'correlation'\n",
    "\n",
    "tra_acc = np.sum(knn_classify(training_data, training_data, training_labels, metric, k) == training_labels) / np.float(len(training_labels))\n",
    "tes_acc = np.sum(knn_classify(test_data, training_data, training_labels, metric, k) == test_labels) / np.float(len(test_labels))\n",
    "\n",
    "print 'Correlation: Training accuracy {}, Test accuracy {}'.format(tra_acc, tes_acc)\n",
    "\n",
    "print 'Testing for k'\n",
    "#We now want to pick the best k, we will go on with cosine distance from now on\n",
    "# We want to choose k large enough to make sure that the noise in the data doesn't distrupt the result\n",
    "# but also small enoguh so that we don't want to begin considering the other classes as a neighbor.\n",
    "#Normally setting k equal to the square root of the size of the set is a good start. In this case\n",
    "#by incrementally decreasing k, I found that 4 was the best balance between training and test accuracy\n",
    "k = 4\n",
    "metric = 'cosine'\n",
    "\n",
    "tra_acc = np.sum(knn_classify(training_data, training_data, training_labels, metric, k) == training_labels) / np.float(len(training_labels))\n",
    "tes_acc = np.sum(knn_classify(test_data, training_data, training_labels, metric, k) == test_labels) / np.float(len(test_labels))\n",
    "\n",
    "print 'K set to square root: Training accuracy {}, Test accuracy {}'.format(tra_acc, tes_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acy 0.8932, Test accuracy 0.906"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
